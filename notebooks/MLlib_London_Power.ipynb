{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.context import SparkContext\n",
    "from pyspark.sql.session import SparkSession\n",
    "#Import Spark CountVectorizer\n",
    "from pyspark.ml.linalg import Vectors\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "from pyspark.ml.feature import CountVectorizer\n",
    "\n",
    "from pyspark.sql.functions import col\n",
    "from pyspark.sql.types import (StructField, DoubleType, StringType,\n",
    "                               IntegerType, TimestampType,\n",
    "                               FloatType, StructType)\n",
    "\n",
    "\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.regression import DecisionTreeRegressor,GBTRegressor,RandomForestRegressor\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "\n",
    "from pyspark.ml.regression import LinearRegression \n",
    "\n",
    "spark = SparkSession \\\n",
    "    .builder \\\n",
    "    .appName(\"mongodb\") \\\n",
    "    .master(\"spark://master:7077\") \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Linear Regression \n",
    "\n",
    "## Importing and preparing the dataset \n",
    "\n",
    "- First we have imported the libraries and ML models we will be using "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_schema = [StructField('LCLid', StringType(), True),\n",
    "              StructField('stdorToU', StringType(), True),\n",
    "              StructField('DateTime', TimestampType(), True),\n",
    "              StructField('KWH/hh (per half hour)',FloatType(), True),\n",
    "              StructField('Acorn', StringType(), True),\n",
    "              StructField('Acorn_grouped', StringType(), True)]\n",
    "\n",
    "final_struc = StructType(fields=data_schema)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- We can load data either from CSV in or Jupyter /work folder that is persistant or we can read the files from HDFS that was prepoulated during spark-submit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "power= spark.read.format(\"csv\").load(\"power.csv\", schema=final_struc, header=True)\n",
    "\n",
    "#power= spark.read.parquet(\"hdfs://hadoop:8020/employees51.parquet\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- We do some cleanup by first renaming the columns to more descriptive titles removing a row that has some odd values and then dropping NA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from pyspark.sql.functions import col\n",
    "# power= power.select(col(\"_c0\").alias(\"LCLid\"), col(\"_c1\").alias(\"stdorToU\"), col(\"_c4\").alias(\"Acorn\"), col(\"_c2\").alias(\"DateTime\"), col(\"_c3\").alias(\"KWH/hh_per_half hour\"), col(\"_c5\").alias(\"Acorn_grouped\"))\n",
    "# power.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#power = power.withColumn(\"KWH/hh_per_half hour\", power[\"KWH/hh_per_half hour\"].cast(DoubleType()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "power=power.filter (power[\"LCLid\"] != 'LCLid')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "power = power.na.drop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Next we encode the columns that have string values and then fitting them to our dataset "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "#categorical veriable encoder another way \n",
    "\n",
    "\n",
    "from pyspark.ml.feature import StringIndexer\n",
    "indexer = StringIndexer(inputCol=\"LCLid\", outputCol=\"LCL_cat\")\n",
    "indexer2 = StringIndexer(inputCol=\"Acorn_grouped\", outputCol=\"Acorn_cat\")\n",
    "indexed = indexer.fit(power).transform(power)\n",
    "indexed = indexer2.fit(indexed).transform(indexed)\n",
    "indexed.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Next we assemble the newly encoded features to a vector  then to save space we choose just the columns we will be using KWH for lable that we want to predict and features dense vector column and call our final dataset final_power "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assembler = VectorAssembler(\n",
    "  inputCols=['LCL_cat','Acorn_cat'],\n",
    "    outputCol=\"features\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = assembler.transform(indexed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output.select(\"features\", \"KWH/hh_per_half hour\").show(500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_power=output.select(\"features\", \"KWH/hh_per_half hour\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_power.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Next we split our dataset final_power to train and test sets 80% to train and 20% to test "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data,test_data =final_power.randomSplit([0.8,0.2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Next we apply Linear regression to our dataset and train our model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = LinearRegression(labelCol='KWH/hh_per_half hour',featuresCol='features')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit the model to the data and call this model lrModel\n",
    "lrModel = lr.fit(train_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Lets see our test resultes. We use evaluate function to create the test_results and in next cell use it to se waht is our Mean Squared Error "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    " test_results = lrModel.evaluate(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.2069325348776791"
      ]
     },
     "execution_count": 133,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_results.meanAbsoluteError\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE: 0.38524109376479526\n",
      "MSE: 0.14841070032509576\n"
     ]
    }
   ],
   "source": [
    "print(\"RMSE: {}\".format(test_results.rootMeanSquaredError))\n",
    "print(\"MSE: {}\".format(test_results.meanSquaredError))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "#only for HDFS\n",
    "from pyspark.sql.types import DoubleType\n",
    "final_power = final_power.withColumn(\"KWH/hh_per_half hour\", final_power[\"KWH/hh_per_half hour\"].cast(DoubleType()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decision tree and Random Forest \n",
    "\n",
    "- We apply three models with default values to our dataset and the in next cell we train the 3 models ...it takes a while "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use mostly defaults to make this comparison \"fair\"\n",
    "\n",
    "dtc = DecisionTreeRegressor(labelCol='KWH/hh_per_half hour',featuresCol='features')\n",
    "rfc = RandomForestRegressor(labelCol='KWH/hh_per_half hour',featuresCol='features')\n",
    "gbt = GBTRegressor(labelCol='KWH/hh_per_half hour',featuresCol='features')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the models (its three models, so it might take some time)\n",
    "dtc_model = dtc.fit(train_data)\n",
    "rfc_model = rfc.fit(train_data)\n",
    "gbt_model = gbt.fit(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [],
   "source": [
    "dtc_predictions = dtc_model.transform(test_data)\n",
    "rfc_predictions = rfc_model.transform(test_data)\n",
    "gbt_predictions = gbt_model.transform(test_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Now to interpret the results we import evaluation library  and in next sell specify our lable, name of our prediction field and metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [],
   "source": [
    "acc_evaluator = MulticlassClassificationEvaluator(labelCol=\"KWH/hh_per_half hour\", predictionCol=\"prediction\", metricName=\"accuracy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [],
   "source": [
    "dtc_acc = acc_evaluator.evaluate(dtc_predictions)\n",
    "rfc_acc = acc_evaluator.evaluate(rfc_predictions)\n",
    "gbt_acc = acc_evaluator.evaluate(gbt_predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Here are the results!\n",
      "--------------------------------------------------------------------------------\n",
      "A single decision tree had an accuracy of: 0.00%\n",
      "--------------------------------------------------------------------------------\n",
      "A random forest ensemble had an accuracy of: 0.00%\n",
      "--------------------------------------------------------------------------------\n",
      "A ensemble using GBT had an accuracy of: 0.00%\n"
     ]
    }
   ],
   "source": [
    "print(\"Here are the results!\")\n",
    "print('-'*80)\n",
    "print('A single decision tree had an accuracy of: {0:2.2f}%'.format(dtc_acc*100))\n",
    "print('-'*80)\n",
    "print('A random forest ensemble had an accuracy of: {0:2.2f}%'.format(rfc_acc*100))\n",
    "print('-'*80)\n",
    "print('A ensemble using GBT had an accuracy of: {0:2.2f}%'.format(gbt_acc*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from pyspark.sql.functions import col, split\n",
    "# power = power.withColumn(\"Acorn_Array\", split(col(\"Acorn_grouped\"),\" \"))\n",
    "\n",
    "#power.show(1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "#AcornVectorizer = CountVectorizer(inputCol=\"Acorn_Array\", outputCol=\"Acorn_OneHotEncoded\", vocabSize=4, minDF=1.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "#AcornVectorizer_model = AcornVectorizer.fit(power)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df_power = AcornVectorizer_model.transform(power)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df_power.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
